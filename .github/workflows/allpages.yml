name: Fetch YouTube Fandom Pages

on:
  # schedule:
  #   - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  fetch-pages:
    runs-on: ubuntu-latest

    env:
      API_URL: "https://youtube.fandom.com/api.php"
      API_PARAMS: "action=query&list=allpages&aplimit=max&format=json"
      OUTPUT_FILE: "allpages.txt"

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v3

      - name: Fetch and Save Pages
        run: |
          #!/bin/bash

          # Initialize variables
          CONTINUE=""
          PAGE_COUNT=0

          while [ -n "$CONTINUE" ] || [ $PAGE_COUNT -eq 0 ]; do
            # Construct the API request with continuation token
            FULL_PARAMS="$API_PARAMS&$CONTINUE"
            API_REQUEST="$API_URL?$FULL_PARAMS"

            # Use curl to fetch data from the API endpoint and append
            curl -s "$API_REQUEST" | jq '.query.allpages[].title' >> "$OUTPUT_FILE"

            # Extract the token from the response
            CONTINUE=$(curl -s "$API_REQUEST" | jq -r '.continue.apcontinue')

            # Check if curl was successful
            if [ $? -eq 0 ]; then
              PAGE_COUNT=$((PAGE_COUNT+1))
              echo "Fetched page batch $PAGE_COUNT"
            else
              echo "Failed to fetch pages from $API_URL"
              exit 1
            fi
          done

          echo "Successfully fetched and saved all pages to $OUTPUT_FILE"